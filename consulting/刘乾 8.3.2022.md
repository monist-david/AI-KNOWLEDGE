Question
===============

1. what if I have two very different dataset during the finetune on the same model. 
2. What is the normal difference between different size of models. 125m - 1.3b - 175b. How much would the performance
   differs and why these numbers?
3. with variations in batch size mostly to obtain increased computational efficiency. How?
    1. increasing the batch size means to have more data put into the computing process once. Then we definitely need
       more computing resources for it. Although the model might converge faster, how much could it actually increase
       the computational efficiency?
4. what type of function is for a neural network? Is it a univariate function or multivariate function?
5. why Stochastic gradient descent can be helpful in escaping the local minimum and finding the global one.
6. What is the difference between computational efficiency and speed?
    1. If the speed is fast, isn't it saving a lot of computational cost?
7. Needs to get clear on gradient descent
8. warm up is to prevent early over-fitting (or any other purpose)
    1. what is early over-fitting (local minimum)?
    2. If we use a larger batch size on our first epoch, will it work the same as warm up process? Because the chance of
       having identical non-related topic samples will be small.
9. What is the biggest factor to influence the performance of a model and the computational cost of a model.
10. language models like OPT, GPT, how many times approximately did they try to train the model in order to get the best
    performance?
11. any good chinese language model
12. what are some hardware failures in training?
13. PALM used part of the parameters for each generation.
14. Although sampling may reduce the repetition issue, but why is there a repetition issue?
15. what does increasing model size bring to the language model?
    1. if larger model size means more possibilities, the number of combinations for languages might is limited, then
       why we need such many parameters to consider such many possibilities?
16. GPT-3 has 570GB data, ERNIE has 4T data, why the size is so different?
    1. is it related to model structure selection, language difference or any other reasons?
    2. Because the Chinese words are much lesser than English words, I supposed English corpus should be bigger than
       Chinese corpus.

Notes
===============



Thoughts with Additional Information
===============

