Question
===============

1. What is the best performance retrieval model?
    1. with the generative model, still need retrieval model?
2. finetune the GPT model, or adding a persona embedding will give better result?
    1. persona embedding might now be able to generate good embedding to represent the user data that are not showing in
       the training dataset, is it true?
    2. Books character might have a lot of different perspectives.
3. why multi-head transformers could capture more features of an input data? Why the large amount of parameters could
   not capture those features?
4. what is the advantage of using log function?
5. https://arxiv.org/pdf/1510.03055.pdf mentioned that As decoding proceeds, the influence of the initial input on
   decoding (i.e., the source sentence representation) diminishes as additional previously-predicted words are encoded
   in the vector representations. Is this the problem only in RNN or also in Transformers. If we are using Transformers,
   the issue that MMI penalizes not only high-frequency, generic responses, but also fluent ones and thus can lead to
   ungrammatical outputs maybe is not an issue anymore?
    1. the length of initial input matters? If T9 depends on the previous T but less on S. But what if the length of the
       initial input increase.
6. different between parameters and hidden neurons. How to calculate the size of a model.
7. when set the max_length = 50, the problem disappear. Why?
8. when you're reading paper, how do you read?
9. Why RNNs using LSTM or gated recurrent unit (GRU) cells can theoretically maintain long-term context in their hidden
   layers, in practice RNNs only use a relatively small part of the history of tokens
10. Is there any case that RNN is still better than transformers?
11. the repetition of the data in the training dataset, is it good or bad?
12. The performance comparison between GPT3 and GPT2

Notes
===============

1. retrieval model is still useful. Because it is controllable. Expandability and controllability are important for a
   business product.
2. For the case that our training dataset is quite small, finetune on GPT is a better way to give the persona of the
   character.
    1. when training an additional embedding to represent the character's persona, we need to make sure the dialogue use
       the persona correctly but not overwhelmingly, the small dataset will largely have the issue of using persona
       overwhelmingly. why?
    2. When the training dataset is too small, the model might just not outputting anything. Why?
3. the reason why I set maximum output length to 50 instead of 20 could make the model generate normal sentences.
    1. 20 words maybe is too small for model to generate things. Because of the small dataset, most of the generated
       responses are over than 20 tokens. So If you set the maximum output be 20 length, it is highly possible that the
       model will generate a EOS (END) token directly because it might be the highest probability, compared with those
       longer but cannot be finished in 20 sentences. There must be some short sentences, why EOS token instead of those
       shorter but make no sense sentences?
4. For the problem that the RNN decoding process the latter generated tokens might influence by the previous generated
   tokens instead of the source. Transformers actually solve this problem nicely. However, in a Transformer model if the
   input context is too large there are less attention distributed to the important tokens from the source, which means
   the model might ignore some key information.
5. The way to read older papers is to understand the problem they solve, why they come up with the problem and why they
   come up with this problem-solving idea. The way they solve the problem might be too old to referenced in this case.
6. Why noy using 4 output neurons instead of 10 for recognizing 10 numbers.
    1. This is not fully expandability so far. However, one way to see it is that 10 output will give more parameters in
       total. Therefore, the model is able to do the task easier and faster.
    2. Although most of the cases, the parameters are too many to complete a task, it is still necessary because we
       couldn't guarantee that we 100% efficiently use all the parameters.
    3. Also, to calculate the number of needed parameters for a certain task is hard.
7. GPT3 might have similar language performance with GPT2, the advantage of it is that it is large enough to do
   zero-shot learning, prompting.

Thoughts with Additional Information
===============

